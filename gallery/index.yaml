---
- url: "github:mudler/LocalAI/gallery/moondream.yaml@master"
  license: apache-2.0
  icon: https://cdn-avatars.huggingface.co/v1/production/uploads/65df6605dba41b152100edf9/LEUWPRTize9N7dMShjcPC.png
  description: |
    Moondream is a small vision language model designed to run efficiently everywhere.
  urls:
    - https://huggingface.co/vikhyatk/moondream2
    - https://huggingface.co/ggml-org/moondream2-20250414-GGUF
  tags:
    - llm
    - multimodal
    - gguf
    - moondream
    - gpu
    - image-to-text
    - vision
    - cpu
  name: "moondream2-20250414"
  overrides:
    mmproj: moondream2-mmproj-f16-20250414.gguf
    parameters:
      model: moondream2-text-model-f16_ct-vicuna.gguf
  files:
    - filename: moondream2-text-model-f16_ct-vicuna.gguf
      sha256: 925bcb666baf69ed747e26121af287b16ae7764483be9548b1382f29783689a5
      uri: https://huggingface.co/ggml-org/moondream2-20250414-GGUF/resolve/main/moondream2-text-model-f16_ct-vicuna.gguf
    - filename: moondream2-mmproj-f16-20250414.gguf
      sha256: 4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
      uri: https://huggingface.co/ggml-org/moondream2-20250414-GGUF/resolve/main/moondream2-mmproj-f16-20250414.gguf
- &smolvlm
  url: "github:mudler/LocalAI/gallery/smolvlm.yaml@master"
  name: "smolvlm-256m-instruct"
  icon: https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/SmolVLM_256_banner.png
  urls:
    - https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct
    - https://huggingface.co/ggml-org/SmolVLM-256M-Instruct-GGUF
  license: apache-2.0
  description: |
    SmolVLM-256M is the smallest multimodal model in the world. It accepts arbitrary sequences of image and text inputs to produce text outputs. It's designed for efficiency. SmolVLM can answer questions about images, describe visual content, or transcribe text. Its lightweight architecture makes it suitable for on-device applications while maintaining strong performance on multimodal tasks. It can run inference on one image with under 1GB of GPU RAM.
  tags:
    - llm
    - gguf
    - gpu
    - cpu
    - vision
    - multimodal
    - smollvlm
    - image-to-text
  overrides:
    parameters:
      model: SmolVLM-256M-Instruct-Q8_0.gguf
    mmproj: mmproj-SmolVLM-256M-Instruct-Q8_0.gguf
  files:
    - filename: mmproj-SmolVLM-256M-Instruct-Q8_0.gguf
      sha256: 7e943f7c53f0382a6fc41b6ee0c2def63ba4fded9ab8ed039cc9e2ab905e0edd
      uri: huggingface://ggml-org/SmolVLM-256M-Instruct-GGUF/mmproj-SmolVLM-256M-Instruct-Q8_0.gguf
    - filename: SmolVLM-256M-Instruct-Q8_0.gguf
      sha256: 2a31195d3769c0b0fd0a4906201666108834848db768af11de1d2cef7cd35e65
      uri: huggingface://ggml-org/SmolVLM-256M-Instruct-GGUF/SmolVLM-256M-Instruct-Q8_0.gguf
- !!merge <<: *smolvlm
  name: "smolvlm2-500m-video-instruct"
  icon: https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/SmolVLM2_banner.png
  urls:
    - https://huggingface.co/HuggingFaceTB/SmolVLM2-500M-Video-Instruct
    - https://huggingface.co/ggml-org/SmolVLM2-500M-Video-Instruct-GGUF
  description: |
    SmolVLM2-500M-Video is a lightweight multimodal model designed to analyze video content.
    The model processes videos, images, and text inputs to generate text outputs - whether answering questions about media files, comparing visual content, or transcribing text from images. Despite its compact size, requiring only 1.8GB of GPU RAM for video inference, it delivers robust performance on complex multimodal tasks.
    This efficiency makes it particularly well-suited for on-device applications where computational resources may be limited.
  overrides:
    parameters:
      model: SmolVLM2-500M-Video-Instruct-f16.gguf
    mmproj: mmproj-SmolVLM2-500M-Video-Instruct-f16.gguf
  files:
    - filename: SmolVLM2-500M-Video-Instruct-f16.gguf
      sha256: 80f7e3f04bc2d3324ac1a9f52f5776fe13a69912adf74f8e7edacf773d140d77
      uri: huggingface://ggml-org/SmolVLM2-500M-Video-Instruct-GGUF/SmolVLM2-500M-Video-Instruct-f16.gguf
    - filename: mmproj-SmolVLM2-500M-Video-Instruct-f16.gguf
      sha256: b5dc8ebe7cbeab66a5369693960a52515d7824f13d4063ceca78431f2a6b59b0
      uri: huggingface://ggml-org/SmolVLM2-500M-Video-Instruct-GGUF/mmproj-SmolVLM2-500M-Video-Instruct-f16.gguf
- &qwen3
  url: "github:mudler/LocalAI/gallery/qwen3.yaml@master"
  name: "qwen3-30b-a3b"
  urls:
    - https://huggingface.co/Qwen/Qwen3-30B-A3B
    - https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF
  icon: https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png
  license: apache-2.0
  description: |
    Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

      Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
      Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
      Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
      Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
      Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.
    Qwen3-30B-A3B has the following features:

        Type: Causal Language Models
        Training Stage: Pretraining & Post-training
        Number of Parameters: 30.5B in total and 3.3B activated
        Number of Paramaters (Non-Embedding): 29.9B
        Number of Layers: 48
        Number of Attention Heads (GQA): 32 for Q and 4 for KV
        Number of Experts: 128
        Number of Activated Experts: 8
        Context Length: 32,768 natively and 131,072 tokens with YaRN.

    For more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.
  tags:
    - llm
    - gguf
    - gpu
    - cpu
    - qwen
    - qwen3
    - thinking
    - reasoning
  overrides:
    parameters:
      model: Qwen_Qwen3-30B-A3B-Q4_K_M.gguf
  files:
    - filename: Qwen_Qwen3-30B-A3B-Q4_K_M.gguf
      sha256: a015794bfb1d69cb03dbb86b185fb2b9b339f757df5f8f9dd9ebdab8f6ed5d32
      uri: huggingface://bartowski/Qwen_Qwen3-30B-A3B-GGUF/Qwen_Qwen3-30B-A3B-Q4_K_M.gguf
- !!merge <<: *qwen3
  name: "qwen3-0.6b"
  urls:
    - https://huggingface.co/Qwen/Qwen3-0.6B
    - https://huggingface.co/MaziyarPanahi/Qwen3-0.6B-GGUF
  description: |
    Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

        Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
        Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
        Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
        Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
        Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.

    Qwen3-0.6B has the following features:

        Type: Causal Language Models
        Training Stage: Pretraining & Post-training
        Number of Parameters: 0.6B
        Number of Paramaters (Non-Embedding): 0.44B
        Number of Layers: 28
        Number of Attention Heads (GQA): 16 for Q and 8 for KV
        Context Length: 32,768
  overrides:
    parameters:
      model: Qwen3-0.6B.Q4_K_M.gguf
  files:
    - filename: Qwen3-0.6B.Q4_K_M.gguf
      uri: https://www.modelscope.cn/models/unsloth/Qwen3-0.6B-GGUF/resolve/master/Qwen3-0.6B-Q4_K_M.gguf
